{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "if 'playscript_utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['playscript_utils'])\n",
    "else:\n",
    "    import playscript_utils\n",
    "from playscript_utils import model_name_dict, emotions, filter_csv_by_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if reader.line_num == 1:\n",
    "                continue\n",
    "            # To convert single-line strings in the CSV with \\\\n to newlines\n",
    "            row_data = row[0].replace('\\\\n', '\\n')\n",
    "            data.append(row_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired with premise_type=\"creative\"; kind of counterintuitive, but just run with it\n",
    "# Think of it as the more creative the premise, the less we need to control the emotion of the characters\n",
    "def format_prompt_generic_emotion(premise, example_playscripts_path):\n",
    "    example_playscripts = read_csv(example_playscripts_path)\n",
    "    prefix = \"Generate a conversation between two characters, Alice and Bob, using the given premise. Here are a few examples:\"\n",
    "    few_shot_prompt = '\\n\\n'.join(example_playscripts[0:3])\n",
    "    suffix = \"Now, generate a conversation between Alice and Bob based on the following premise. Generate exactly 3 lines of dialogue per character, alternating per character, so 6 lines of dialogue in total. Do not include stage directions like '(smirking)' or '(smiling)'. At the end of the conversation, output '[END]'.\"\n",
    "    premise_formatted = f\"Premise: {premise}\"\n",
    "    prompt = '\\n\\n'.join([prefix, few_shot_prompt, suffix, premise_formatted])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired with premise_type=\"neutral\"\n",
    "# Think of it as the more neutral the premise, the more we can control the emotion of the characters\n",
    "def format_prompt_controlled_emotion(premise, alice_emotion, bob_emotion):\n",
    "    alice_emotion_prefix = f\"Alice's dialogue should convey {alice_emotion}. \" if alice_emotion != \"generic\" else \"\"\n",
    "    bob_emotion_prefix = f\"Bob's dialogue should convey {bob_emotion}. \" if bob_emotion != \"generic\" else \"\"\n",
    "    prefix = f\"Generate a conversation between Alice and Bob based on a given premise. Generate exactly 3 lines of dialogue per person, \"\\\n",
    "        f\"alternating per person, so 6 lines of dialogue in total. {alice_emotion_prefix}{bob_emotion_prefix}At the end of the conversation, output '[END]'. \"\\\n",
    "        \"The format of the conversation should look like this:\"\n",
    "    conversation_example = \"Alice: ...\\nBob: ...\\nAlice: ...\\nBob: ...\\nAlice: ...\\nBob: ...\\n[END]\"\n",
    "    suffix = \"Now, generate a conversation based on the following premise:\"\n",
    "    premise_formatted = f\"Premise: {premise}\"\n",
    "    prompt = '\\n\\n'.join([prefix, conversation_example, suffix, premise_formatted])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_HF(model, tokenizer, prompt, num_generations=4, temperature=0.7):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        model_gen_tokens = model.generate(input_ids, max_new_tokens=256, min_new_tokens=5, temperature=temperature, num_return_sequences=num_generations)[:, input_ids.shape[-1]:]\n",
    "        generations = []\n",
    "        for i in range(num_generations):\n",
    "            generations.append(tokenizer.decode(model_gen_tokens[i], skip_special_tokens=True).strip())\n",
    "        return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_openai(client, model_name, prompt, num_generations=1, temperature=1.0, seed=42):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=256,\n",
    "        n=num_generations,\n",
    "        temperature=temperature,\n",
    "        seed=seed\n",
    "    )\n",
    "    generations = [completion.message.content for completion in completion.choices]\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_playscript(unfiltered_playscript):\n",
    "    end_index = unfiltered_playscript.find(\"[END]\")\n",
    "    if end_index != -1:\n",
    "        filtered_playscript = unfiltered_playscript[:end_index].split('\\n')\n",
    "    else:\n",
    "        filtered_playscript = unfiltered_playscript.split('\\n')\n",
    "    filtered_playscript = [line for line in filtered_playscript if line.strip() != '']\n",
    "    for i in range(len(filtered_playscript)):\n",
    "        if \"Alice: \" in filtered_playscript[i] or \"Bob: \" in filtered_playscript[i]:\n",
    "            filtered_playscript = filtered_playscript[i:]\n",
    "            break\n",
    "    dialogues = []\n",
    "    for i in range(len(filtered_playscript)):\n",
    "        current_character = \"Alice\" if i % 2 == 0 else \"Bob\"\n",
    "        if f'{current_character}: ' not in filtered_playscript[i]:\n",
    "            raise ValueError(f\"The current line doesn't start with {current_character}: {filtered_playscript[i]}\")\n",
    "        character, dialogue = filtered_playscript[i].split(\": \", 1)\n",
    "        dialogues.append(dialogue)\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_playscripts_csv(generated_playscripts_path, generation_model_name, temperature, seed, num_generations, premise_type, premise, alice_emotion, bob_emotion, unfiltered_playscript, dialogues, dialogues_length, error_message):\n",
    "    unfiltered_playscript_single_line = unfiltered_playscript.replace('\\n', '\\\\n')\n",
    "    with open(generated_playscripts_path, mode='a+', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"generation_model\", \"temperature\", \"seed\", \"num_generations\", \"premise_type\", \"premise\", \"alice_emotion\", \"bob_emotion\", \"unfiltered_playscript\", \"dialogues\", \"dialogues_length\", \"error_message\"])\n",
    "        else:\n",
    "            # Read the existing rows and replace the first row\n",
    "            file.seek(0)\n",
    "            existing_rows = list(csv.reader(file))\n",
    "            existing_rows[0] = [\"generation_model\", \"temperature\", \"seed\", \"num_generations\", \"premise_type\", \"premise\", \"alice_emotion\", \"bob_emotion\", \"unfiltered_playscript\", \"dialogues\", \"dialogues_length\", \"error_message\"]\n",
    "            file.seek(0)\n",
    "            file.truncate()\n",
    "            writer.writerows(existing_rows)\n",
    "        writer.writerow([generation_model_name, temperature, seed, num_generations, premise_type, premise, alice_emotion, bob_emotion, unfiltered_playscript_single_line, dialogues, dialogues_length, error_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_playscripts(premise_type, generation_model_type, generation_model_name, num_generations, temperature, seed, example_playscripts_path, generated_playscripts_path, premises_path, emotions=None, model=None, tokenizer=None, client=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    premises_df = pd.read_csv(premises_path)\n",
    "    filtered_premises_df = premises_df[premises_df['premise_type'].eq(premise_type)]\n",
    "    premise_list = filtered_premises_df['premise'].tolist()\n",
    "    for i, premise in enumerate(premise_list):\n",
    "        print(f\"Generating playscript for premise {i+1} of {len(premises_df)}: {premise}\")\n",
    "        if premise_type == \"creative\":\n",
    "            formatted_prompt = format_prompt_generic_emotion(premise, example_playscripts_path)\n",
    "            if generation_model_type == \"HF\":\n",
    "                unfiltered_playscripts = generate_text_HF(model, tokenizer, formatted_prompt, num_generations, temperature)\n",
    "            elif generation_model_type == \"openai\":\n",
    "                unfiltered_playscripts = generate_text_openai(client, generation_model_name, formatted_prompt, num_generations, temperature, seed)\n",
    "            for unfiltered_playscript in unfiltered_playscripts:\n",
    "                try:\n",
    "                    dialogues = filter_playscript(unfiltered_playscript)\n",
    "                    append_to_playscripts_csv(generated_playscripts_path, generation_model_name, temperature, seed, num_generations, premise_type, premise, \"generic\", \"generic\", unfiltered_playscript, dialogues, len(dialogues), \"\")\n",
    "                except ValueError as e:\n",
    "                    append_to_playscripts_csv(generated_playscripts_path, generation_model_name, temperature, seed, num_generations, premise_type, premise, \"generic\", \"generic\", unfiltered_playscript, \"\", \"\", str(e))\n",
    "        elif premise_type == \"neutral\":\n",
    "            emotion_combinations = [[emotion1, emotion2] for emotion1 in emotions for emotion2 in emotions]\n",
    "            emotion_combinations.insert(0, [\"generic\", \"generic\"])\n",
    "            for alice_emotion, bob_emotion in emotion_combinations:\n",
    "                formatted_prompt = format_prompt_controlled_emotion(premise, alice_emotion, bob_emotion)\n",
    "                if generation_model_type == \"HF\":\n",
    "                    unfiltered_playscripts = generate_text_HF(model, tokenizer, formatted_prompt, num_generations, temperature)\n",
    "                elif generation_model_type == \"openai\":\n",
    "                    unfiltered_playscripts = generate_text_openai(client, generation_model_name, formatted_prompt, num_generations, temperature, seed)\n",
    "                for unfiltered_playscript in unfiltered_playscripts:\n",
    "                    try:\n",
    "                        dialogues = filter_playscript(unfiltered_playscript)\n",
    "                        append_to_playscripts_csv(generated_playscripts_path, generation_model_name, temperature, seed, num_generations, premise_type, premise, alice_emotion, bob_emotion, unfiltered_playscript, dialogues, len(dialogues), \"\")\n",
    "                    except ValueError as e:\n",
    "                        append_to_playscripts_csv(generated_playscripts_path, generation_model_name, temperature, seed, num_generations, premise_type, premise, alice_emotion, bob_emotion, unfiltered_playscript, \"\", \"\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_generated_playscripts(generated_playscripts_path, edited_playscripts_path):\n",
    "# Manual editing of row 83, which threw an error when generating (for generic emotion with Llama3_70B)\n",
    "    with open(generated_playscripts_path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Replace row 83 with row 82 (index 82 with 81 in 0-based index)\n",
    "    rows[82] = rows[81]\n",
    "\n",
    "    # Save the edited version to a new CSV file\n",
    "    with open(edited_playscripts_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_format(read_file_path, write_file_path):\n",
    "    df = pd.read_csv(read_file_path)\n",
    "    df.rename(columns={'model_name': 'generation_model'}, inplace=True)\n",
    "    # df['probing_model'] = \"llama2_13b_chat\"\n",
    "    # df['probing_method'] = \"pca\"\n",
    "    # Reorder columns to match the exact order\n",
    "    # df = df[['generation_model', 'temperature', 'seed', 'num_generations', 'premise_type', 'premise', 'alice_emotion', 'bob_emotion', 'unfiltered_conversation', 'dialogues', 'dialogues_length', 'error_message', 'probing_model', 'probing_method', 'stimulis_format', 'dialogue_concatenate', 'emotion_scores']]\n",
    "    df = df[['generation_model', 'temperature', 'seed', 'num_generations', 'premise_type', 'premise', 'alice_emotion', 'bob_emotion', 'unfiltered_conversation', 'dialogues', 'dialogues_length', 'error_message']]\n",
    "\n",
    "    df.to_csv(write_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_path = \"../data/controlled/generated_playscripts.csv\"\n",
    "write_file_path = \"../data/temp/generated_playscripts.csv\"\n",
    "convert_csv_format(read_file_path, write_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating playscripts with generic emotion\n",
    "example_playscripts_path = \"../data/temp/example_playscripts.csv\"\n",
    "generated_playscripts_path = \"../data/temp/generated_playscripts.csv\"\n",
    "premises_path = \"../data/temp/premises.csv\"\n",
    "generation_model_name = \"llama3_8b_instruct\"\n",
    "# generation_model_HF = model_name_dict[generation_model_name]\n",
    "# generation_model = AutoModelForCausalLM.from_pretrained(generation_model_HF, device_map=\"auto\", low_cpu_mem_usage = True, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(generation_model_HF, trust_remote_code=True)\n",
    "\n",
    "# generate_playscripts(\"creative\", \"HF\", generation_model_name, 4, 0.7, 42, example_playscripts_path, generated_playscripts_path, premises_path, emotions, generation_model, tokenizer, client=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating playscript for premise 1 of 1: Alice and Bob grow herbs in their apartments. They discuss herb gardening.\n"
     ]
    }
   ],
   "source": [
    "# Generating playscripts with controlled emotion\n",
    "generation_model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])   \n",
    "generate_playscripts(\"neutral\", \"openai\", model_name, 1, 1, 42, example_playscripts_path, generated_playscripts_path, premises_path, emotions=None, model=None, tokenizer=None, client=client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rep_eng_retry_kernel",
   "language": "python",
   "name": "rep_eng_retry_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
