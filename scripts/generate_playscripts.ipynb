{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    \"llama2_13b_chat\": \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama3_8b\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"llama3_8b_instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama3_70b\": \"meta-llama/Meta-Llama-3-70B\",\n",
    "    \"llama3_70b_instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if reader.line_num == 1:\n",
    "                continue\n",
    "            row_data = row[0].replace('\\\\n', '\\n')\n",
    "            data.append(row_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(premise, example_playscripts_path):\n",
    "    example_playscripts = read_csv(example_playscripts_path)\n",
    "    prefix = \"Generate a conversation between two characters, Alice and Bob, using the given premise. Here are a few examples:\"\n",
    "    few_shot_prompt = '\\n\\n'.join(example_playscripts[0:3])\n",
    "    suffix = \"Now, generate a conversation between Alice and Bob based on the following premise. Generate exactly 3 lines of dialogue per character, alternating per character, so 6 lines of dialogue in total. Do not include stage directions like '(smirking)' or '(smiling)'. At the end of the conversation, output '[END]'.\"\n",
    "    premise_formatted = f\"Premise: {premise}\"\n",
    "    prompt = '\\n\\n'.join([prefix, few_shot_prompt, suffix, premise_formatted])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model_name, prompt, num_generations=1, temperature=0.7):\n",
    "    model_HF = model_name_dict[model_name]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_HF, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_HF, device_map=\"auto\", low_cpu_mem_usage = True, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        model_gen_tokens = model.generate(input_ids, max_new_tokens=256, min_new_tokens=5, temperature=temperature, num_return_sequences=num_generations)[:, input_ids.shape[-1]:]\n",
    "        generations = []\n",
    "        for i in range(num_generations):\n",
    "            generations.append(tokenizer.decode(model_gen_tokens[i], skip_special_tokens=True).strip())\n",
    "        return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conversation_by_character(unfiltered_conversation):\n",
    "    end_index = unfiltered_conversation.find(\"[END]\")\n",
    "    if end_index != -1:\n",
    "        filtered_conversation = unfiltered_conversation[:end_index].split('\\n')\n",
    "    else:\n",
    "        filtered_conversation = unfiltered_conversation.split('\\n')\n",
    "    filtered_conversation = [line for line in filtered_conversation if line.strip() != '']\n",
    "    for i in range(len(filtered_conversation)):\n",
    "        if \"Alice: \" in filtered_conversation[i] or \"Bob: \" in filtered_conversation[i]:\n",
    "            filtered_conversation = filtered_conversation[i:]\n",
    "            break\n",
    "    character_dialogues = {}\n",
    "    character_dialogues[\"Alice\"] = []\n",
    "    character_dialogues[\"Bob\"] = []\n",
    "    for i in range(len(filtered_conversation)):\n",
    "        current_character = \"Alice\" if i % 2 == 0 else \"Bob\"\n",
    "        if f'{current_character}: ' not in filtered_conversation[i]:\n",
    "            raise ValueError(f\"The current line doesn't start with {current_character}: {filtered_conversation[i]}\")\n",
    "        character, dialogue = filtered_conversation[i].split(\": \", 1)\n",
    "        character_dialogues[character].append(dialogue)\n",
    "    return character_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_playscripts_csv(generated_playscripts_path, model_name, temperature, seed, num_generations, premise, unfiltered_conversation, alice_dialogues, bob_dialogues, error_message):\n",
    "    unfiltered_conversation_single_line = unfiltered_conversation.replace('\\n', '\\\\n')\n",
    "    with open(generated_playscripts_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"model_name\", \"temperature\", \"seed\", \"num_generations\", \"premise\", \"unfiltered_conversation\", \"alice_dialogues\", \"bob_dialogues\", \"error_message\"])\n",
    "        writer.writerow([model_name, temperature, seed, num_generations, premise, unfiltered_conversation_single_line, alice_dialogues, bob_dialogues, error_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_playscripts(premises, model_name, num_generations=4, temperature=0.7, seed=42, example_playscripts_path=\"data/example_playscripts.csv\", generated_playscripts_path=\"data/generated_playscripts.csv\"):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    for i, premise in enumerate(premises):\n",
    "        print(f\"Generating playscript {i+1} of {len(premises)}\")\n",
    "        unfiltered_conversations = []\n",
    "        unfiltered_conversations = generate_text(model_name, format_prompt(premise, example_playscripts_path), num_generations=num_generations, temperature=temperature)\n",
    "        for unfiltered_conversation in unfiltered_conversations:\n",
    "            try:\n",
    "                character_dialogues = filter_conversation_by_character(unfiltered_conversation)\n",
    "                alice_dialogues = character_dialogues[\"Alice\"]\n",
    "                bob_dialogues = character_dialogues[\"Bob\"]\n",
    "                append_to_playscripts_csv(generated_playscripts_path, model_name, temperature, seed, num_generations, premise, unfiltered_conversation, alice_dialogues, bob_dialogues, \"\")\n",
    "            except ValueError as e:\n",
    "                append_to_playscripts_csv(generated_playscripts_path, model_name, temperature, seed, num_generations, premise, unfiltered_conversation, \"\", \"\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_playscripts_path = \"data/example_playscripts.csv\"\n",
    "generated_playscripts_path = \"data/generated_playscripts.csv\"\n",
    "test_premises_path = \"data/test_premises.csv\"\n",
    "model_name = \"llama3_8b_instruct\"\n",
    "\n",
    "test_premises = read_csv(test_premises_path)[:1]\n",
    "generate_playscripts(test_premises, model_name, num_generations=4, temperature=0.7, seed=42, example_playscripts_path=example_playscripts_path, generated_playscripts_path=generated_playscripts_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual editing of row 83, which threw an error when generating\n",
    "with open(generated_playscripts_path, mode='r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Replace row 83 with row 82 (index 82 with 81 in 0-based index)\n",
    "rows[82] = rows[81]\n",
    "\n",
    "# Save the edited version to a new CSV file\n",
    "generated_playscripts_edited_path = \"data/generated_playscripts_edited.csv\"\n",
    "with open(generated_playscripts_edited_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rep_eng_retry_kernel",
   "language": "python",
   "name": "rep_eng_retry_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
