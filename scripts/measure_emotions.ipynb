{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('representation-engineering')\n",
    "sys.path.append('representation-engineering/examples/primary_emotions')\n",
    "from repe import repe_pipeline_registry\n",
    "repe_pipeline_registry()\n",
    "\n",
    "from utils import primary_emotions_concept_dataset\n",
    "from playscript_utils import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    \"llama2_13b_chat\": \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama3_8b\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"llama3_8b_instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama3_70b\": \"meta-llama/Meta-Llama-3-70B\",\n",
    "    \"llama3_70b_instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a03d18e3e449698627d7c36d24bd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model setup\n",
    "\n",
    "model_name = \"llama2_13b_chat\"\n",
    "\n",
    "model_HF = model_name_dict[model_name]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_HF, torch_dtype=torch.float16, device_map=\"auto\", token=True).eval()\n",
    "# use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False, token=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_HF, padding_side=\"left\", legacy=False, token=True)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_playscripts(generated_playscripts_path, emotions, user_tag, assistant_tag):\n",
    "    generated_playscripts = pd.read_csv(generated_playscripts_path)\n",
    "    alice_data = {emotion: {\"test\": {\"data\": []}} for emotion in emotions}\n",
    "    bob_data = {emotion: {\"test\": {\"data\": []}} for emotion in emotions}\n",
    "\n",
    "    alice_data_origins = {}\n",
    "    bob_data_origins = {}\n",
    "    template_str = '{user_tag} Consider the {emotion} of the following scenario:\\nScenario: {scenario}\\nAnswer: {assistant_tag} '\n",
    "\n",
    "    for premise_id, row in generated_playscripts.iterrows():\n",
    "        alice_dialogues = eval(row[\"alice_dialogues\"])\n",
    "        bob_dialogues = eval(row[\"bob_dialogues\"])\n",
    "        for dialogue_id, dialogue in enumerate(alice_dialogues):\n",
    "            for emotion in emotions:\n",
    "                formatted_dialogue = template_str.format(user_tag=user_tag, emotion=emotion, scenario=dialogue, assistant_tag=assistant_tag)\n",
    "                alice_data[emotion]['test']['data'].append(formatted_dialogue)\n",
    "                alice_data_origins[formatted_dialogue] = {\"premise id\": premise_id, \"dialogue id\": dialogue_id, \"emotion\": emotion}\n",
    "                \n",
    "        for dialogue_id, dialogue in enumerate(bob_dialogues):\n",
    "            for emotion in emotions:\n",
    "                formatted_dialogue = template_str.format(user_tag=user_tag, emotion=emotion, scenario=dialogue, assistant_tag=assistant_tag)\n",
    "                bob_data[emotion]['test']['data'].append(formatted_dialogue)\n",
    "                bob_data_origins[formatted_dialogue] = {\"premise id\": premise_id, \"dialogue id\": dialogue_id, \"emotion\": emotion}\n",
    "    return alice_data, bob_data, alice_data_origins, bob_data_origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_rep_readers(emotions, rep_reading_pipeline, data, rep_token, hidden_layers, n_difference, direction_method):\n",
    "    emotion_rep_readers = {}\n",
    "\n",
    "    for emotion in tqdm(emotions):\n",
    "        train_data = data[emotion]['train']\n",
    "        rep_reader = rep_reading_pipeline.get_directions(\n",
    "            train_data['data'], \n",
    "            rep_token=rep_token, \n",
    "            hidden_layers=hidden_layers, \n",
    "            n_difference=n_difference, \n",
    "            train_labels=train_data['labels'], \n",
    "            direction_method=direction_method,\n",
    "        )\n",
    "        emotion_rep_readers[emotion] = rep_reader\n",
    "    return emotion_rep_readers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_emotion_H_tests(emotions, emotion_rep_readers, rep_reading_pipeline, data, rep_token, hidden_layers):\n",
    "    emotion_H_tests = {}\n",
    "    for emotion in tqdm(emotions):\n",
    "        test_data = data[emotion]['test']\n",
    "        rep_reader = emotion_rep_readers[emotion]\n",
    "        H_tests = rep_reading_pipeline(\n",
    "            test_data['data'],\n",
    "            rep_token=rep_token, \n",
    "            hidden_layers=hidden_layers, \n",
    "            rep_reader=rep_reader,\n",
    "            batch_size=32)\n",
    "        emotion_H_tests[emotion] = H_tests\n",
    "        print(\"finished H_tests for emotion\", emotion)\n",
    "    return emotion_H_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_accuracy(emotions, emotion_rep_readers, emotion_H_tests, data, hidden_layers):\n",
    "    results = {layer: {} for layer in hidden_layers}\n",
    "    for layer in hidden_layers:\n",
    "        for emotion in emotions:\n",
    "            test_data = data[emotion]['test']\n",
    "            sign = emotion_rep_readers[emotion].direction_signs[layer].item()\n",
    "\n",
    "            # # Original metric method (pairwise relative)\n",
    "            # H_test = [H[layer] for H in emotion_H_tests[emotion]] \n",
    "            # H_test = [H_test[i:i+2] for i in range(0, len(H_test), 2)]\n",
    "            # eval_func = min if sign == -1 else max\n",
    "            # cors = np.mean([eval_func(H) == H[0] for H in H_test])\n",
    "\n",
    "            # # Modified metric method (Absolute with 0 boundary)\n",
    "            # H_test = [H[layer] * sign for H in emotion_H_tests[emotion]] \n",
    "            # cors = np.mean([(H_test[i] > 0) == (data[emotion]['test']['labels'][0][i] == 1) for i in range(len(H_test))])\n",
    "\n",
    "            # Modified metric method (Absolute with average boundary)\n",
    "            H_test = [H[layer] * sign for H in emotion_H_tests[emotion]] \n",
    "            avg_H_test = np.mean(H_test)\n",
    "            cors = np.mean([(H_test[i] > avg_H_test) == (test_data['labels'][0][i] == 1) for i in range(len(H_test))])\n",
    "\n",
    "            results[layer][emotion] = cors\n",
    "            \n",
    "    for emotion in emotions:\n",
    "        x = list(results.keys())\n",
    "        y = [results[layer][emotion] for layer in results]\n",
    "\n",
    "        plt.plot(x, y, label=emotion)\n",
    "\n",
    "    plt.title(\"Emotions Acc\")\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_scores(emotions, emotion_rep_readers, emotion_H_tests, data, layer):\n",
    "    emotion_scores = {emotion: [] for emotion in emotions}\n",
    "    for emotion in emotions:\n",
    "        test_data = data[emotion]['test']\n",
    "        sign = emotion_rep_readers[emotion].direction_signs[layer].item()\n",
    "        H_test = [H[layer] * sign for H in emotion_H_tests[emotion]] \n",
    "        avg_H_test = np.mean(H_test)\n",
    "        for i in range(len(test_data['data'])):\n",
    "            emotion_scores[emotion].append((test_data['data'][i], H_test[i]))\n",
    "    return emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_emotion_scores(generated_playscripts_path, emotion_metrics_path, emotions, alice_emotion_scores, alice_data_origins, bob_emotion_scores, bob_data_origins):\n",
    "    df = pd.read_csv(generated_playscripts_path)\n",
    "\n",
    "    # Write Alice's emotion scores\n",
    "    # Initialize the new columns\n",
    "    df['alice_emotion_scores'] = [{} for _ in range(len(df))]\n",
    "    # Populate the df with empty <emotion, []> in each dictionary\n",
    "    for i in range(len(df)):\n",
    "        alice_dialogues = eval(str(df.at[i, f'alice_dialogues']))\n",
    "\n",
    "        alice_dialogues_length = len(alice_dialogues)\n",
    "        for emotion in emotions:\n",
    "            df.at[i, f'alice_emotion_scores'][emotion] = [0] * alice_dialogues_length\n",
    "\n",
    "    for emotion in emotions:\n",
    "        for formatted_dialogue, emotion_score in alice_emotion_scores[emotion]:\n",
    "            origin_info = alice_data_origins[formatted_dialogue]\n",
    "            premise_id = origin_info[\"premise id\"]\n",
    "            dialogue_id = origin_info[\"dialogue id\"]\n",
    "            df.at[premise_id, f'alice_emotion_scores'][emotion][dialogue_id] = emotion_score\n",
    "\n",
    "    # Write Bob's emotion scores\n",
    "    # Initialize the new columns\n",
    "    df['bob_emotion_scores'] = [{} for _ in range(len(df))]\n",
    "    # Populate the df with empty <emotion, []> in each dictionary\n",
    "    for i in range(len(df)):\n",
    "        bob_dialogues = eval(str(df.at[i, f'bob_dialogues']))\n",
    "\n",
    "        bob_dialogues_length = len(bob_dialogues)\n",
    "        for emotion in emotions:\n",
    "            df.at[i, f'bob_emotion_scores'][emotion] = [0] * bob_dialogues_length\n",
    "\n",
    "    for emotion in emotions:\n",
    "        for formatted_dialogue, emotion_score in bob_emotion_scores[emotion]:\n",
    "            origin_info = bob_data_origins[formatted_dialogue]\n",
    "            premise_id = origin_info[\"premise id\"]\n",
    "            dialogue_id = origin_info[\"dialogue id\"]\n",
    "            df.at[premise_id, f'bob_emotion_scores'][emotion][dialogue_id] = emotion_score\n",
    "\n",
    "    df.to_csv(emotion_metrics_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_emotion_metrics():\n",
    "    rep_reading_pipeline = pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)\n",
    "    rep_token = -1\n",
    "    hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))\n",
    "    n_difference = 1\n",
    "    direction_method = 'pca'\n",
    "    emotions = [\"happiness\", \"sadness\", \"anger\", \"fear\", \"disgust\", \"surprise\"]\n",
    "    user_tag =  \"[INST]\"\n",
    "    assistant_tag =  \"[/INST]\"\n",
    "\n",
    "    baseline_data_dir = \"representation-engineering/data/emotions\"\n",
    "    generated_playscripts_path = \"data/generated_playscripts.csv\"\n",
    "    emotion_metrics_path = \"data/emotion_metrics.csv\"\n",
    "\n",
    "    baseline_data = primary_emotions_concept_dataset(baseline_data_dir, user_tag=user_tag, assistant_tag=assistant_tag)\n",
    "    alice_data, bob_data, alice_data_origins, bob_data_origins = get_generated_playscripts(generated_playscripts_path, emotions, user_tag, assistant_tag)\n",
    "    emotion_rep_readers = get_emotion_rep_readers(emotions, rep_reading_pipeline, baseline_data, rep_token, hidden_layers, n_difference, direction_method)\n",
    "    alice_emotion_H_tests = run_emotion_H_tests(emotions, emotion_rep_readers, rep_reading_pipeline, alice_data, rep_token, hidden_layers)\n",
    "    bob_emotion_H_tests = run_emotion_H_tests(emotions, emotion_rep_readers, rep_reading_pipeline, bob_data, rep_token, hidden_layers)\n",
    "    alice_emotion_scores = get_emotion_scores(emotions, emotion_rep_readers, alice_emotion_H_tests, alice_data, -1)\n",
    "    bob_emotion_scores = get_emotion_scores(emotions, emotion_rep_readers, bob_emotion_H_tests, bob_data, -1)\n",
    "\n",
    "    write_emotion_scores(generated_playscripts_path, emotion_metrics_path, \"alice\", emotions, alice_emotion_scores, alice_data_origins)\n",
    "    write_emotion_scores(generated_playscripts_path, emotion_metrics_path, \"bob\", emotions, bob_emotion_scores, bob_data_origins)\n",
    "    \n",
    "    # emotion_H_tests = run_emotion_H_tests(emotions, emotion_rep_readers, rep_reading_pipeline, baseline_data, rep_token, hidden_layers)\n",
    "    # baseline_emotion_scores = get_emotion_scores(emotions, emotion_rep_readers, emotion_H_tests, baseline_data, -1)\n",
    "    # evaluate_classification_accuracy(emotions, emotion_rep_readers, emotion_H_tests, baseline_data, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:51<00:00,  8.60s/it]\n",
      " 17%|█▋        | 1/6 [00:05<00:25,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion happiness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:09<00:19,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:14<00:14,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:18<00:09,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:23<00:04,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:27<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:04<00:21,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion happiness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:08<00:17,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:13<00:13,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:17<00:08,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:22<00:04,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:26<00:00,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished H_tests for emotion surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rep_reading_pipeline = pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)\n",
    "rep_token = -1\n",
    "hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))\n",
    "n_difference = 1\n",
    "direction_method = 'pca'\n",
    "emotions = [\"happiness\", \"sadness\", \"anger\", \"fear\", \"disgust\", \"surprise\"]\n",
    "user_tag =  \"[INST]\"\n",
    "assistant_tag =  \"[/INST]\"\n",
    "\n",
    "baseline_data_dir = \"representation-engineering/data/emotions\"\n",
    "generated_playscripts_path = \"data/generated_playscripts_edited.csv\"\n",
    "emotion_metrics_path = \"data/emotion_metrics.csv\"\n",
    "\n",
    "baseline_data = primary_emotions_concept_dataset(baseline_data_dir, user_tag=user_tag, assistant_tag=assistant_tag)\n",
    "alice_data, bob_data, alice_data_origins, bob_data_origins = get_generated_playscripts(generated_playscripts_path, emotions, user_tag, assistant_tag)\n",
    "emotion_rep_readers = get_emotion_rep_readers(emotions, rep_reading_pipeline, baseline_data, rep_token, hidden_layers, n_difference, direction_method)\n",
    "alice_emotion_H_tests = run_emotion_H_tests(emotions, emotion_rep_readers, rep_reading_pipeline, alice_data, rep_token, hidden_layers)\n",
    "bob_emotion_H_tests = run_emotion_H_tests(emotions, emotion_rep_readers, rep_reading_pipeline, bob_data, rep_token, hidden_layers)\n",
    "alice_emotion_scores = get_emotion_scores(emotions, emotion_rep_readers, alice_emotion_H_tests, alice_data, -1)\n",
    "bob_emotion_scores = get_emotion_scores(emotions, emotion_rep_readers, bob_emotion_H_tests, bob_data, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_emotion_scores(generated_playscripts_path, emotion_metrics_path, emotions, alice_emotion_scores, alice_data_origins, bob_emotion_scores, bob_data_origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rep_eng_retry_kernel",
   "language": "python",
   "name": "rep_eng_retry_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
