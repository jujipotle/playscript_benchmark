{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace representation-engineering/rep_readers.py with the following code to implement logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "import torch\n",
    "\n",
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    # Calculate the magnitude of the direction vector\n",
    "     # Ensure H and direction are on the same device (CPU or GPU)\n",
    "    if type(direction) != torch.Tensor:\n",
    "        H = torch.Tensor(H).cuda()\n",
    "    if type(direction) != torch.Tensor:\n",
    "        direction = torch.Tensor(direction)\n",
    "        direction = direction.to(H.device)\n",
    "    mag = torch.norm(direction)\n",
    "    assert not torch.isinf(mag).any()\n",
    "    # Calculate the projection\n",
    "    projection = H.matmul(direction) / mag\n",
    "    return projection\n",
    "\n",
    "def recenter(x, mean=None):\n",
    "    x = torch.Tensor(x).cuda()\n",
    "    if mean is None:\n",
    "        mean = torch.mean(x,axis=0,keepdims=True).cuda()\n",
    "    else:\n",
    "        mean = torch.Tensor(mean).cuda()\n",
    "    return x - mean\n",
    "\n",
    "class RepReader(ABC):\n",
    "    \"\"\"Class to identify and store concept directions.\n",
    "    \n",
    "    Subclasses implement the abstract methods to identify concept directions \n",
    "    for each hidden layer via strategies including PCA, embedding vectors \n",
    "    (aka the logits method), and cluster means.\n",
    "\n",
    "    RepReader instances are used by RepReaderPipeline to get concept scores.\n",
    "\n",
    "    Directions can be used for downstream interventions.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        self.direction_method = None\n",
    "        self.directions = None # directions accessible via directions[layer][component_index]\n",
    "        self.direction_signs = None # direction of high concept scores (mapping min/max to high/low)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_rep_directions(self, model, tokenizer, hidden_states, hidden_layers, **kwargs):\n",
    "        \"\"\"Get concept directions for each hidden layer of the model\n",
    "        \n",
    "        Args:\n",
    "            model: Model to get directions for\n",
    "            tokenizer: Tokenizer to use\n",
    "            hidden_states: Hidden states of the model on the training data (per layer)\n",
    "            hidden_layers: Layers to consider\n",
    "\n",
    "        Returns:\n",
    "            directions: A dict mapping layers to direction arrays (n_components, hidden_size)\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def get_signs(self, hidden_states, train_choices, hidden_layers):\n",
    "        \"\"\"Given labels for the training data hidden_states, determine whether the\n",
    "        negative or positive direction corresponds to low/high concept \n",
    "        (and return corresponding signs -1 or 1 for each layer and component index)\n",
    "        \n",
    "        NOTE: This method assumes that there are 2 entries in hidden_states per label, \n",
    "        aka len(hidden_states[layer]) == 2 * len(train_choices). For example, if \n",
    "        n_difference=1, then hidden_states here should be the raw hidden states\n",
    "        rather than the relative (i.e. the differences between pairs of examples).\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Hidden states of the model on the training data (per layer)\n",
    "            train_choices: Labels for the training data\n",
    "            hidden_layers: Layers to consider\n",
    "\n",
    "        Returns:\n",
    "            signs: A dict mapping layers to sign arrays (n_components,)\n",
    "        \"\"\"        \n",
    "        signs = {}\n",
    "\n",
    "        if self.needs_hiddens and hidden_states is not None and len(hidden_states) > 0:\n",
    "            for layer in hidden_layers:    \n",
    "                assert hidden_states[layer].shape[0] == 2 * len(train_choices), f\"Shape mismatch between hidden states ({hidden_states[layer].shape[0]}) and labels ({len(train_choices)})\"\n",
    "                # BEGIN EDIT\n",
    "                layer_signs = torch.Tensor(np.zeros(self.n_components)).cpu()\n",
    "                # END EDIT\n",
    "                for component_index in range(self.n_components):\n",
    "                    transformed_hidden_states = project_onto_direction(hidden_states[layer], self.directions[layer][component_index])\n",
    "                    projected_scores = [transformed_hidden_states[i:i+2] for i in range(0, len(transformed_hidden_states), 2)]\n",
    "\n",
    "                    outputs_min = [1 if min(o) == o[label] else 0 for o, label in zip(projected_scores, train_choices)]\n",
    "                    outputs_max = [1 if max(o) == o[label] else 0 for o, label in zip(projected_scores, train_choices)]\n",
    "                # BEGIN EDIT\n",
    "                    layer_signs[component_index] = -1 if np.mean(outputs_min) > np.mean(outputs_max) else 1\n",
    "                signs[layer] = layer_signs\n",
    "                # END EDIT\n",
    "        else:\n",
    "            for layer in hidden_layers:    \n",
    "                signs[layer] = [1 for _ in range(self.n_components)]\n",
    "\n",
    "        return signs\n",
    "\n",
    "\n",
    "    def transform(self, hidden_states, hidden_layers, component_index):\n",
    "        \"\"\"Project the hidden states onto the concept directions in self.directions\n",
    "\n",
    "        Args:\n",
    "            hidden_states: dictionary with entries of dimension (n_examples, hidden_size)\n",
    "            hidden_layers: list of layers to consider\n",
    "            component_index: index of the component to use from self.directions\n",
    "\n",
    "        Returns:\n",
    "            transformed_hidden_states: dictionary with entries of dimension (n_examples,)\n",
    "        \"\"\"\n",
    "\n",
    "        assert component_index < self.n_components\n",
    "        transformed_hidden_states = {}\n",
    "        for layer in hidden_layers:\n",
    "            layer_hidden_states = hidden_states[layer]\n",
    "            # BEGIN EDIT - don't recenter\n",
    "            # if hasattr(self, 'H_train_means'):\n",
    "            #     layer_hidden_states = recenter(layer_hidden_states, mean=self.H_train_means[layer])\n",
    "            # # ---\n",
    "            # else:\n",
    "            #     print(\"no attr\")\n",
    "            # END EDIT\n",
    "            layer_hidden_states = layer_hidden_states.to(torch.float32)\n",
    "            # project hidden states onto found concept directions (e.g. onto PCA comp 0) \n",
    "            # BEGIN EDIT - transform using bias\n",
    "            # H_transformed = project_onto_direction(layer_hidden_states, self.directions[layer][component_index])\n",
    "            direction = torch.from_numpy(self.directions[layer][component_index].T).cuda()\n",
    "            if hasattr(self, 'biases'):\n",
    "                bias = torch.from_numpy(self.biases[layer]).cuda()\n",
    "            else:\n",
    "                bias = torch.zeros(1).cuda()\n",
    "            H_transformed = layer_hidden_states.matmul(direction) + bias\n",
    "            # END EDIT\n",
    "            transformed_hidden_states[layer] = H_transformed.cpu().numpy()       \n",
    "        return transformed_hidden_states\n",
    "    \n",
    "class LogisticRegressionRepReader(RepReader):\n",
    "    \"\"\"Extract directions via logistic regression\"\"\"\n",
    "    needs_hiddens = True \n",
    "\n",
    "    def __init__(self, n_components=1):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.H_train_means = {}\n",
    "        self.directions = {}\n",
    "        self.biases = {}\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def get_rep_directions(self, model, tokenizer, hidden_states, hidden_layers, **kwargs):\n",
    "        \"\"\"Get logistic regression directions for each layer\"\"\"\n",
    "        directions = {}\n",
    "        train_choices = kwargs['train_choices']\n",
    "        # BEGIN EDIT - we already format the labels to be a single dimension array\n",
    "        # train_choices = np.concatenate(train_choices)\n",
    "        # END EDIT\n",
    "        assert train_choices is not None, \"LogisticRegressionRepReader requires train_choices to differentiate classes\"\n",
    "        for layer in hidden_layers:\n",
    "            assert len(train_choices) == len(hidden_states[layer]), f\"Shape mismatch between hidden states ({len(hidden_states[layer])}) and labels ({len(train_choices)})\"\n",
    "        \n",
    "        for layer in hidden_layers:\n",
    "            H_train = hidden_states[layer]\n",
    "            # BEGIN EDIT - don't recenter\n",
    "            # H_train_mean = H_train.mean(axis=0, keepdims=True)\n",
    "            # self.H_train_means[layer] = H_train_mean\n",
    "            # H_train = recenter(H_train, mean=H_train_mean).cpu()\n",
    "            # END EDIT\n",
    "            H_train = np.vstack(H_train)\n",
    "            y_train = train_choices\n",
    "            \n",
    "            clf = LogisticRegression(random_state=0, max_iter=1000).fit(H_train, y_train)\n",
    "            directions[layer] = clf.coef_ # shape (1, n_features)\n",
    "            self.directions[layer] = clf.coef_\n",
    "            # BEGIN EDIT - set bias\n",
    "            self.biases[layer] = clf.intercept_\n",
    "            # END EDIT\n",
    "            self.classifiers[layer] = clf\n",
    "        return directions\n",
    "\n",
    "    def get_signs(self, hidden_states, train_labels, hidden_layers):\n",
    "        signs = {layer: np.ones(self.n_components) for layer in hidden_layers}\n",
    "        return signs\n",
    "\n",
    "class PCARepReader(RepReader):\n",
    "    \"\"\"Extract directions via PCA\"\"\"\n",
    "    needs_hiddens = True \n",
    "\n",
    "    def __init__(self, n_components=1):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.H_train_means = {}\n",
    "\n",
    "    def get_rep_directions(self, model, tokenizer, hidden_states, hidden_layers, **kwargs):\n",
    "        \"\"\"Get PCA components for each layer\"\"\"\n",
    "        directions = {}\n",
    "\n",
    "        for layer in hidden_layers:\n",
    "            H_train = hidden_states[layer]\n",
    "            H_train_mean = H_train.mean(axis=0, keepdims=True)\n",
    "            self.H_train_means[layer] = H_train_mean\n",
    "            H_train = recenter(H_train, mean=H_train_mean).cpu()\n",
    "            H_train = np.vstack(H_train)\n",
    "            pca_model = PCA(n_components=self.n_components, whiten=False).fit(H_train)\n",
    "\n",
    "            directions[layer] = pca_model.components_ # shape (n_components, n_features)\n",
    "            self.n_components = pca_model.n_components_\n",
    "        \n",
    "        return directions\n",
    "\n",
    "    def get_signs(self, hidden_states, train_labels, hidden_layers):\n",
    "\n",
    "        signs = {}\n",
    "\n",
    "        for layer in hidden_layers:\n",
    "            assert hidden_states[layer].shape[0] == len(np.concatenate(train_labels)), f\"Shape mismatch between hidden states ({hidden_states[layer].shape[0]}) and labels ({len(np.concatenate(train_labels))})\"\n",
    "            layer_hidden_states = hidden_states[layer]\n",
    "\n",
    "            # NOTE: since scoring is ultimately comparative, the effect of this is moot\n",
    "            layer_hidden_states = recenter(layer_hidden_states, mean=self.H_train_means[layer])\n",
    "\n",
    "            # get the signs for each component\n",
    "            layer_signs = np.zeros(self.n_components)\n",
    "            for component_index in range(self.n_components):\n",
    "\n",
    "                transformed_hidden_states = project_onto_direction(layer_hidden_states, self.directions[layer][component_index]).cpu()\n",
    "                \n",
    "                pca_outputs_comp = [list(islice(transformed_hidden_states, sum(len(c) for c in train_labels[:i]), sum(len(c) for c in train_labels[:i+1]))) for i in range(len(train_labels))]\n",
    "\n",
    "                # We do elements instead of argmin/max because sometimes we pad random choices in training\n",
    "                pca_outputs_min = np.mean([o[train_labels[i].index(1)] == min(o) for i, o in enumerate(pca_outputs_comp)])\n",
    "                pca_outputs_max = np.mean([o[train_labels[i].index(1)] == max(o) for i, o in enumerate(pca_outputs_comp)])\n",
    "\n",
    "       \n",
    "                layer_signs[component_index] = np.sign(np.mean(pca_outputs_max) - np.mean(pca_outputs_min))\n",
    "                if layer_signs[component_index] == 0:\n",
    "                    layer_signs[component_index] = 1 # default to positive in case of tie\n",
    "\n",
    "            signs[layer] = layer_signs\n",
    "\n",
    "        return signs\n",
    "    \n",
    "\n",
    "        \n",
    "class ClusterMeanRepReader(RepReader):\n",
    "    \"\"\"Get the direction that is the difference between the mean of the positive and negative clusters.\"\"\"\n",
    "    n_components = 1\n",
    "    needs_hiddens = True\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_rep_directions(self, model, tokenizer, hidden_states, hidden_layers, **kwargs):\n",
    "\n",
    "        # train labels is necessary to differentiate between different classes\n",
    "        train_choices = kwargs['train_choices'] if 'train_choices' in kwargs else None\n",
    "        assert train_choices is not None, \"ClusterMeanRepReader requires train_choices to differentiate two clusters\"\n",
    "        for layer in hidden_layers:\n",
    "            assert len(train_choices) == len(hidden_states[layer]), f\"Shape mismatch between hidden states ({len(hidden_states[layer])}) and labels ({len(train_choices)})\"\n",
    "\n",
    "        train_choices = np.array(train_choices)\n",
    "        neg_class = np.where(train_choices == 0)\n",
    "        pos_class = np.where(train_choices == 1)\n",
    "\n",
    "        directions = {}\n",
    "        for layer in hidden_layers:\n",
    "            H_train = np.array(hidden_states[layer])\n",
    "\n",
    "            H_pos_mean = H_train[pos_class].mean(axis=0, keepdims=True)\n",
    "            H_neg_mean = H_train[neg_class].mean(axis=0, keepdims=True)\n",
    "\n",
    "            directions[layer] = H_pos_mean - H_neg_mean\n",
    "        \n",
    "        return directions\n",
    "\n",
    "\n",
    "class RandomRepReader(RepReader):\n",
    "    \"\"\"Get random directions for each hidden layer. Do not use hidden \n",
    "    states or train labels of any kind.\"\"\"\n",
    "\n",
    "    def __init__(self, needs_hiddens=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_components = 1\n",
    "        self.needs_hiddens = needs_hiddens\n",
    "\n",
    "    def get_rep_directions(self, model, tokenizer, hidden_states, hidden_layers, **kwargs):\n",
    "\n",
    "        directions = {}\n",
    "        for layer in hidden_layers:\n",
    "            directions[layer] = np.expand_dims(np.random.randn(model.config.hidden_size), 0)\n",
    "\n",
    "        return directions\n",
    "\n",
    "\n",
    "DIRECTION_FINDERS = {\n",
    "    'pca': PCARepReader,\n",
    "    'cluster_mean': ClusterMeanRepReader,\n",
    "    'random': RandomRepReader,\n",
    "    'logistic_regression': LogisticRegressionRepReader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: If you wish to play around with the representation engineering emotion classification notebook, in representation-engineering/examples/primary_emotions/utils.py, replace primary_emotions_concept_dataset() with the following code to split the train and test data in 50/50 ratio (by default, they are the same, which is weird):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_emotions_concept_dataset(data_dir, user_tag='', assistant_tag='', seed=0):\n",
    "    random.seed(0)\n",
    "\n",
    "    template_str = '{user_tag} Consider the {emotion} of the following scenario:\\nScenario: {scenario}\\nAnswer: {assistant_tag} '\n",
    "    emotions = [\"happiness\", \"sadness\", \"anger\", \"fear\", \"disgust\", \"surprise\"]\n",
    "    raw_data = {}\n",
    "    for emotion in emotions:\n",
    "        with open(os.path.join(data_dir, f'{emotion}.json')) as file:\n",
    "            # raw_data[emotion] = json.load(file)\n",
    "            raw_data[emotion] = list(set(json.load(file)))[:200]\n",
    "\n",
    "    formatted_data = {}\n",
    "    for emotion in emotions:\n",
    "        c_e, o_e = raw_data[emotion], np.concatenate([v for k,v in raw_data.items() if k != emotion])\n",
    "        random.shuffle(o_e)\n",
    "\n",
    "        data = [[c,o] for c,o in zip(c_e, o_e)]\n",
    "        train_labels = []\n",
    "        for d in data:\n",
    "            true_s = d[0]\n",
    "            random.shuffle(d)\n",
    "            train_labels.append([s == true_s for s in d])\n",
    "        \n",
    "        data = np.concatenate(data).tolist()\n",
    "        data_ = np.concatenate([[c,o] for c,o in zip(c_e, o_e)]).tolist()\n",
    "        \n",
    "        emotion_test_data = [template_str.format(emotion=emotion, scenario=d, user_tag=user_tag, assistant_tag=assistant_tag) for d in data_]\n",
    "        emotion_train_data = [template_str.format(emotion=emotion, scenario=d, user_tag=user_tag, assistant_tag=assistant_tag) for d in data]\n",
    "\n",
    "        formatted_data[emotion] = {\n",
    "            'train': {'data': emotion_train_data[:200], 'labels': train_labels[:100]},\n",
    "            'test': {'data': emotion_test_data[200:], 'labels': [[1,0]* len(emotion_test_data[200:])]}\n",
    "            # 'train': {'data': emotion_train_data, 'labels': train_labels},\n",
    "            # 'test': {'data': emotion_test_data, 'labels': [[1,0]* len(emotion_test_data)]}\n",
    "        }\n",
    "    return formatted_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
